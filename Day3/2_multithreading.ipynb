{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia starts with a single thread. We must tell it explicitly to start multiple threads.\n",
    "\n",
    "#### Environmental variable\n",
    "\n",
    "On Linux/MacOS:\n",
    "\n",
    "```bash\n",
    "export JULIA_NUM_THREADS=4\n",
    "```\n",
    "\n",
    "On Windows:\n",
    "\n",
    "```bash\n",
    "set JULIA_NUM_THREADS=4\n",
    "```\n",
    "\n",
    "Afterwards start julia.\n",
    "\n",
    "#### Command line argument\n",
    "\n",
    "```bash\n",
    "julia -t 4\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```bash\n",
    "julia --threads 4\n",
    "```\n",
    "\n",
    "#### Jupyter kernel\n",
    "\n",
    "You can also create a *Jupyter kernel* for multithreaded Julia:\n",
    "\n",
    "```julia\n",
    "using IJulia\n",
    "installkernel(\"Julia (4 threads)\", env=Dict(\"JULIA_NUM_THREADS\"=>\"4\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily check how many threads we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in contrast to our distributed computing discussion, we will only consider only a single process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Distributed: nprocs\n",
    "\n",
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Threads.@spawn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Threads.@spawn` macro is very similar to the `Distributed.@spawn` macro. However, instead of spawning tasks on different worker processes (potentially on different machines) it spawns tasks on different threads. Basically, it creates (and immediately returns) a `Task` and puts it onto a todo-list. The scheduler will then dynamically assign these tasks to threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import everything we need from `Base.Threads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base.Threads: @spawn, nthreads, threadid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@spawn println(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `Threads.@spawn` returns the task right away, we might have to wait until the task is done and then fetch our result (again, just as for `Distributed.@spawn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = @spawn begin sleep(3); return 4 end # returns right away\n",
    "@time fetch(t) # we need to wait until the task is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use (some of) the control flow tools that we've already seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync t = @spawn begin sleep(3); return 4 end # only returns the task once it is done\n",
    "@time fetch(t) # no need to wait, the task is already done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can spawn many tasks in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:2*nthreads()\n",
    "    @spawn println(\"Hi, I'm \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tasks are **dynamically scheduled**. Some threads do more work (more values of i) than others!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `@threads`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly speaking, the macro `Threads.@threads` is for multithreading (threads) what `Distributed.@distributed` was for distributed computing (processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base.Threads: @threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads for i in 1:2*nthreads()\n",
    "    println(\"Hi, I'm \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to `@spawn` above, `@threads` is **statically scheduled**. The iteration range of the for loop is divided equally between the threads. Every thread handles precisely two iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In a future version of Julia, there will likely be other scheduling policies, for example a `:dynamic` similar to the behavior of `@spawn`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @threads :static for i in 1:2*nthreads()\n",
    "#     println(\"Hi, I'm \", threadid())\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is my `pmap` pendant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, it doesn't exist. But, conceptually, we can implement it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(fn, itr) = map(fetch, map(i -> Threads.@spawn(fn(i)), itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(i -> println(i, \" ($(threadid()))\"), 1:10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that this implementation creates temporary allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `@threads` vs `@spawn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `@threads`\n",
    "\n",
    "* rather lightweight, i.e. small overhead\n",
    "* **statically scheduled**\n",
    "* good for uniform workload\n",
    "\n",
    "#### `@spawn`\n",
    "\n",
    "* more overhead\n",
    "* **dynamically scheduled**\n",
    "* handles non-uniform workloads well\n",
    "* efficient nesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: filling an array in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal ist to fill an array in parallel using `@threads` and `@spawn`, respectively.\n",
    "\n",
    "Note that while we had to use things like `SharedArrays` in the case of distributed computing, threads share the same memory! We also don't have to use constructs like `@everywhere` since everything is running in the same julia process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base.Threads: @threads, @spawn, nthreads, threadid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fill_array_threads(a)\n",
    "    @threads for i in 1:length(a)\n",
    "        a[i] = threadid()\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zeros(nthreads()*10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_array_threads(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a version using `Threads.@spawn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fill_array_spawn(a)\n",
    "    @sync for i in 1:length(a)\n",
    "        @spawn(a[i] = threadid())\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zeros(nthreads()*10);\n",
    "fill_array_spawn(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark both variants!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "@btime fill_array_threads(x) samples=10 setup=(x = zeros(nthreads()*10));\n",
    "@btime fill_array_spawn(x) samples=10 setup=(x = zeros(nthreads()*10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `@threads` based implementation is much faster. However, this shouldn't be surprising, given that our tasks are lightweight and uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-uniform workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fill_array_threads_nonuniform(a)\n",
    "    @threads for i in 1:length(a)\n",
    "        # calculate the squared magnitude of random vectors growing cubically in size\n",
    "        a[i] = sum(abs2, rand() for j in 1:(i^3))\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zeros(nthreads()*10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_array_threads_nonuniform(a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fill_array_spawn_nonuniform(a)\n",
    "    @sync for i in 1:length(a)\n",
    "        Threads.@spawn(a[i] = sum(abs2, rand() for j in 1:(i^3)))\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_array_spawn_nonuniform(a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "@btime fill_array_threads_nonuniform(x) samples=10 setup=(x = zeros(nthreads()*10));\n",
    "@btime fill_array_spawn_nonuniform(x) samples=10 setup=(x = zeros(nthreads()*10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that compared to above, the result has flipped: the `@spawn` implementation, due to it's dynamic scheduling, is more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threads require more care: parallel summation (naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mysum(xs)\n",
    "    s = zero(eltype(xs))\n",
    "    for x in xs\n",
    "        s += x\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mysum_threaded_naive(xs)\n",
    "    s = zero(eltype(xs))\n",
    "    @threads for x in xs\n",
    "        s += x\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = rand(100_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sum(xs);\n",
    "@show mysum(xs);\n",
    "@show mysum_threaded_naive(xs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel summation (divide the work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mysum_threaded(xs)\n",
    "    b = ceil(Int, length(xs)/nthreads())\n",
    "    map(sub_xs -> Threads.@spawn(sum(sub_xs)), Iterators.partition(xs, b)) .|> fetch |> sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sum(xs);\n",
    "@show mysum(xs);\n",
    "@show mysum_threaded(xs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "@btime mysum($xs);\n",
    "@btime mysum_threaded($xs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel summation (atomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base.Threads: Atomic, atomic_add!\n",
    "\n",
    "function mysum_threaded_atomics(xs)\n",
    "    s = Atomic{eltype(xs)}(zero(eltype(xs)))\n",
    "    @threads for x in xs\n",
    "        atomic_add!(s, x)\n",
    "    end\n",
    "    return s[]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show mysum(xs);\n",
    "@show mysum_threaded_atomics(xs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime mysum(xs);\n",
    "@btime mysum_threaded_atomics(xs);\n",
    "@btime mysum_threaded(xs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Atomic Operations](https://docs.julialang.org/en/v1/manual/parallel-computing/#Atomic-Operations-1) in the Julia doc for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spawning of tasks can also be nested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threaded_fun()\n",
    "    x = threadid()\n",
    "    @spawn println(\"job1\", \" (spawned from $x, processed by $(threadid()))\")\n",
    "    @spawn println(\"job2\", \" (spawned from $x, processed by $(threadid()))\")\n",
    "    @spawn println(\"job3\", \" (spawned from $x, processed by $(threadid()))\")\n",
    "end\n",
    "\n",
    "for i in 1:nthreads()\n",
    "    @spawn threaded_fun()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threaded_fun2()\n",
    "    x = threadid()\n",
    "    @threads for i in 1:3\n",
    "        println(\"job$i\", \" (spawned from $x, processed by $(threadid()))\")\n",
    "    end\n",
    "end\n",
    "\n",
    "@threads for i in 1:nthreads()\n",
    "    threaded_fun2()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
