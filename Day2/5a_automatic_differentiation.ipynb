{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation (AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know how to take derivatives.\n",
    "```julia\n",
    "f(x) = 5*x^2 + 3\n",
    "\n",
    "df(x) = 10*x\n",
    "\n",
    "ddf(x) = 10\n",
    "```\n",
    "\n",
    "The promise of AD is\n",
    "\n",
    "```julia\n",
    "df(x) = derivative(f, x)\n",
    "\n",
    "ddf(x) = derivative(df, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What AD is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://www.jmlr.org/papers/volume18/17-468/17-468.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symbolic differentiation:** (at least not exactly)\n",
    "$$ \\frac{d}{dx}x^n = n x^{n-1}. $$\n",
    "\n",
    "**Numerical differentiation:**\n",
    "$$ \\frac{df}{dx} \\approx \\frac{f(x+h) - f(x)}{\\Delta h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key to AD is the application of the chain rule\n",
    "$$\\dfrac{d}{dx} f(g(x)) = \\dfrac{df}{dg} \\dfrac{dg}{dx}$$\n",
    "\n",
    "Consider the function $f(a,b) = \\ln(ab + \\sin(a))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b) = log(a*b + sin(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_derivative(a,b) = 1/(a*b + sin(a)) * (b + cos(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3.1\n",
    "b = 2.4\n",
    "f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the function into the elementary steps, it corresponds to the following \"*computational graph*\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/comp_graph.svg\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_graph(a,b)\n",
    "    c1 = a*b\n",
    "    c2 = sin(a)\n",
    "    c3 = c1 + c2\n",
    "    c4 = log(c3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b) == f_graph(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $\\frac{\\partial f}{\\partial a}$ we have to apply the chain rule multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial f}{\\partial a} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial a} = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\dfrac{\\partial c_3}{\\partial a}  \\right) = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\left( \\dfrac{\\partial c_3}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial a} + \\dfrac{\\partial c_3}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial a}\\right)  \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_graph_derivative(a,b)\n",
    "    c1 = a*b\n",
    "    c1_ϵ = b\n",
    "    \n",
    "    c2 = sin(a)\n",
    "    c2_ϵ = cos(a)\n",
    "    \n",
    "    c3 = c1 + c2\n",
    "    c3_ϵ = c1_ϵ + c2_ϵ\n",
    "    \n",
    "    c4 = log(c3)\n",
    "    c4_ϵ = 1/c3 * c3_ϵ\n",
    "    \n",
    "    c4, c4_ϵ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_graph_derivative(a,b)[2] == f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we automate this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D for \"dual number\", invented by Clifford in 1873.\n",
    "struct D <: Number\n",
    "    x::Float64 # value\n",
    "    ϵ::Float64 # derivative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: +, *, /, -, sin, log, convert, promote_rule\n",
    "\n",
    "a::D + b::D = D(a.x + b.x, a.ϵ + b.ϵ) # sum rule\n",
    "a::D - b::D = D(a.x - b.x, a.ϵ - b.ϵ)\n",
    "a::D * b::D = D(a.x * b.x, a.x * b.ϵ + a.ϵ * b.x) # product rule\n",
    "a::D / b::D = D(a.x / b.x, (b.x * a.ϵ - a.x * b.ϵ)/b.x^2) # quotient rule\n",
    "sin(a::D) = D(sin(a.x), cos(a.x) * a.ϵ)\n",
    "log(a::D) = D(log(a.x), 1/a.x * a.ϵ)\n",
    "\n",
    "Base.convert(::Type{D}, x::Real) = D(x, zero(x))\n",
    "Base.promote_rule(::Type{D}, ::Type{<:Number}) = D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! That was easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(D(a,1), b).ϵ ≈ f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this work?!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick of forward mode AD is to make the computer do the rewrite `f -> f_graph_derivative` for you (and then optimize the resulting code structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_typed f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is somewhat hard to parse, plugging these operations manually into each other we find that this code equals\n",
    "\n",
    "```julia\n",
    "D.x = log(a.x*b + sin(a.x))\n",
    "D.ϵ = 1/(a.x*b + sin(a.x)) * (a.x*0 + (a.ϵ*b) + cos(a.x)*a.ϵ)\n",
    "```\n",
    "\n",
    "which, if we drop `a.x*0`, set `a.ϵ = 1`, and rename `a.x` $\\rightarrow$ `a`, reads\n",
    "\n",
    "```julia\n",
    "D.x = log(a*b + sin(a))\n",
    "D.ϵ = 1/(a*b + sin(a)) * (b + cos(a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This precisely matches our definitions from above:\n",
    "\n",
    "```julia\n",
    "f(a,b) = log(a*b + sin(a))\n",
    "\n",
    "f_derivative(a,b) = 1/(a*b + sin(a)) * (b + cos(a))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, the compiler sees the entire \"rewritten\" code and can therefore apply optimizations. In this simple example, we find that the code produced by our simple Forward mode AD is essentially identical to the explicit implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f_graph_derivative(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for our small forward AD\n",
    "derivative(f::Function, x::Number) = f(D(x, one(x))).ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->f(x,b), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->3*x^2+4x+5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->sin(x)*log(x), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x) = derivative(a->f(a,b),x) # partial derivative wrt a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(1.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking derivatives of code: Babylonian sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Repeat $t \\leftarrow (t + x/2)/2$ until $t$ converges to $\\sqrt{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function Babylonian(x; N = 10)\n",
    "    t = (1+x)/2\n",
    "    for i = 2:N\n",
    "        t = (t + x/t)/2\n",
    "    end\n",
    "    t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(2), √2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "xs = 0:0.01:49\n",
    "\n",
    "p = plot(title = \"Those Babylonians really knew what they were doing\")\n",
    "for i in 1:5\n",
    "    plot!(p, xs, [Babylonian(x; N=i) for x in xs], label=\"Iteration $i\")\n",
    "end\n",
    "\n",
    "plot!(p, xs, sqrt.(xs), label=\"sqrt\", color=:black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and now the derivative, automagically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same babylonian algorithm with no rewrite at all computes properly the derivative as the check shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(D(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "√5, 0.5 / √5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It just works and is efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none Babylonian(D(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursion? Works as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(x, n) = n <= 0 ? 1 : x*pow(x, n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x -> pow(x,3), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving our Vandermonde matrix from yesterday? Sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vander_generic(x::AbstractVector{T}) where T\n",
    "    m = length(x)\n",
    "    V = Matrix{T}(undef, m, m)\n",
    "    for j = 1:m\n",
    "        V[j,1] = one(x[j])\n",
    "    end\n",
    "    for i= 2:m\n",
    "        for j = 1:m\n",
    "            V[j,i] = x[j] * V[j,i-1]\n",
    "            end\n",
    "        end\n",
    "    return V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = 2, 3, 4, 5\n",
    "V = vander_generic([D(a,1), D(b,1), D(c,1), D(d,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x->getfield(x, :ϵ)).(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolically (because we can)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is mathematically equivalent, **though not exactly what the computation is doing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols(\"x\")\n",
    "\n",
    "# display(\"Iterations as a function of x\")\n",
    "# for k = 1:5\n",
    "#     display(simplify(Babylonian(x; N=k)))\n",
    "# end\n",
    "\n",
    "display(\"Derivatives as a function of x\")\n",
    "for k = 1:5\n",
    "    display(simplify(diff(simplify(Babylonian(x; N=k)), x)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none Babylonian(D(5, 1); N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForwardDiff.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have understood how forward AD works, we can use the more feature complete package [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@edit ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: [DiffRules.jl](https://github.com/JuliaDiff/DiffRules.jl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode:\n",
    "$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\dfrac{\\partial c_3}{\\partial x}  \\right) = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\left( \\dfrac{\\partial c_3}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial x} + \\dfrac{\\partial c_3}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial x}\\right)  \\right)$\n",
    "\n",
    "Reverse mode:\n",
    "$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial x} = \\left( \\dfrac{\\partial f}{\\partial c_3}\\dfrac{\\partial c_3}{\\partial c_4}   \\right) \\dfrac{\\partial c_4}{\\partial x} = \\left( \\left( \\dfrac{\\partial f}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial c_3} + \\dfrac{\\partial f}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial c_3} \\right) \\dfrac{\\partial c_3}{\\partial c_4} \\right) \\dfrac{\\partial c_4}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode AD requires $n$ passes in order to compute an $n$-dimensional\n",
    "gradient.\n",
    "\n",
    "Reverse mode AD requires only a single run in order to compute a complete gradient but requires two passes through the graph: a forward pass during which necessary intermediate values are computed and a backward pass which computes the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rule of thumb:*\n",
    "\n",
    "Forward mode is good for $\\mathbb{R} \\rightarrow \\mathbb{R}^n$ while reverse mode is good for $\\mathbb{R}^n \\rightarrow \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An efficient source-to-source reverse mode AD is implemented in [Zygote.jl](https://github.com/FluxML/Zygote.jl), the AD underlying [Flux.jl](https://fluxml.ai/) (since version 0.10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 5*x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(f, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none gradient(f,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some nice reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectures:\n",
    "\n",
    "* https://mitmath.github.io/18337/lecture8/automatic_differentiation.html\n",
    "\n",
    "Blog posts:\n",
    "\n",
    "* ML in Julia: https://julialang.org/blog/2018/12/ml-language-compiler\n",
    "\n",
    "* Nice example: https://fluxml.ai/2019/03/05/dp-vs-rl.html\n",
    "\n",
    "* Nice interactive examples: https://fluxml.ai/experiments/\n",
    "\n",
    "* Why Julia for ML? https://julialang.org/blog/2017/12/ml&pl\n",
    "\n",
    "* Neural networks with differential equation layers: https://julialang.org/blog/2019/01/fluxdiffeq\n",
    "\n",
    "* Implement Your Own Automatic Differentiation with Julia in ONE day : http://blog.rogerluo.me/2018/10/23/write-an-ad-in-one-day/\n",
    "\n",
    "* Implement Your Own Source To Source AD in ONE day!: http://blog.rogerluo.me/2019/07/27/yassad/\n",
    "\n",
    "Repositories:\n",
    "\n",
    "* AD flavors, like forward and reverse mode AD: https://github.com/MikeInnes/diff-zoo (Mike is one of the smartest Julia ML heads)\n",
    "\n",
    "Talks:\n",
    "\n",
    "* AD is a compiler problem: https://juliacomputing.com/assets/pdf/CGO_C4ML_talk.pdf"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
